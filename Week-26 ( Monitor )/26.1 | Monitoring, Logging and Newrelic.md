# 26.1 | Monitoring, Logging and Newrelic

Let's delve into the concepts of logging, monitoring, alerts, and status pages in detail, exploring their purpose, functionalities, and importance in maintaining robust IT systems.

### 1. Logging

**Logging** is the process of capturing and storing detailed records of events or actions taken by an application, system, or service. Logs provide a trail of activity that can be invaluable for diagnosing issues, tracking user behavior, and auditing system actions. Here are the common types of logs:

- **Error Messages**: Logs that capture details about errors or exceptions occurring in the system. These logs typically include the error type, message, stack trace, timestamp, and other contextual information. They are crucial for debugging and understanding the cause of failures.

- **Access Logs**: These logs record details of access attempts to resources or services, such as who accessed what and when. This is important for security and auditing, providing a clear record of user interactions with the system.

- **Audit Logs**: Detailed logs for compliance and security purposes that track changes to data, configurations, and system states. They are often used in regulated industries to ensure transparency and accountability, showing who made changes, when, and what was changed.

- **Transaction Logs**: Logs that record the details of transactions within an application, often used in financial systems or databases to ensure data integrity and to enable recovery in case of a failure.

- **Debug Logs**: Detailed logs that include diagnostic information useful for developers when troubleshooting application issues. These logs are typically more verbose and may include information like function calls, variable values, and processing steps.

### 2. Monitoring

**Monitoring** refers to the continuous observation of a system's performance and health to ensure it is functioning correctly and efficiently. Monitoring systems collect data about various aspects of an environment and provide insights into system behavior. Key metrics tracked in monitoring include:

- **CPU Usage**: Measures the percentage of CPU capacity being used by the application or system. High CPU usage can indicate a need for performance optimization or additional resources.

- **Memory Usage**: Tracks the amount of RAM being used. High memory usage might lead to swapping and eventually impact system performance.

- **Disk I/O**: Observes the read and write operations on storage devices. High I/O rates can indicate potential bottlenecks or performance issues related to disk access.

- **Disk Space**: Monitors the total amount of storage available and how much is currently being used. Insufficient disk space can lead to system crashes and data loss.

- **Network Traffic**: Measures data transfer rates and overall network activity. This can help identify bandwidth issues, potential bottlenecks, or unauthorized data transfers.

- **Application Performance**: Involves monitoring key performance indicators like response times, throughput (amount of data processed in a given time), and error rates. This is crucial for maintaining a high-quality user experience and identifying performance degradation.

### 3. Alerts

**Alerts** are automated notifications that inform administrators or stakeholders about significant events or issues in the system, based on predefined thresholds or conditions. Alerts are a critical component of proactive system management, allowing for timely intervention before minor issues escalate into major problems. Examples of situations that might trigger alerts include:

- **System Downtime**: An alert is sent when a system or service becomes unavailable. This allows for quick action to restore service and minimize downtime.

- **High CPU Usage**: An alert is triggered when CPU usage exceeds a certain threshold, indicating potential performance issues that need to be addressed.

- **Memory Exhaustion**: An alert is issued if available memory drops below a set threshold, which could lead to system instability if not resolved.

- **Error Spike**: A sudden increase in error rates could indicate a new bug or a system malfunction. Alerts help in quickly identifying and resolving such issues.

Alerts can be sent through various channels, including email, SMS, messaging platforms like Slack, or paging systems, depending on the urgency and the preferences of the team.

### 4. Status Pages

**Status Pages** provide real-time information about the current operational state of a service or system. They are used to communicate the status of services to users, stakeholders, or customers, especially during outages or incidents. Status pages typically include:

- **Service Status**: Indications of whether each service is operational, experiencing issues, or down. This helps users understand the impact of any disruptions.

- **Incident Reports**: Detailed descriptions of current or past incidents, including what is affected, what is being done to resolve the issue, and estimated recovery times.

- **Performance Metrics**: Display of relevant metrics that give insights into the system's health, such as uptime percentages, response times, or error rates.

- **Maintenance Notices**: Announcements of planned maintenance activities that might affect service availability. This helps manage user expectations and reduce the impact of planned outages.

- **Historical Data**: Access to past incidents and their resolutions, providing transparency and a track record of system reliability.

Examples of status pages include:
- **[100xDevs Status Page](https://status.100xdevs.com/)**: This page provides real-time information about the status of 100xDevs services.
- **[Backpack Exchange Status Page](https://status.backpack.exchange/)**: Displays the current operational status and any ongoing incidents affecting Backpack Exchange services.

Status pages are a valuable tool for maintaining trust and transparency with users by providing clear, timely information about service health and incidents.

### Importance of Logging, Monitoring, Alerts, and Status Pages

- **Proactive Maintenance**: By continuously monitoring systems and setting up alerts, organizations can proactively address issues before they impact users, reducing downtime and improving reliability.

- **Troubleshooting and Debugging**: Logs provide detailed information about system operations, making it easier to diagnose and fix problems quickly.

- **Compliance and Security**: Audit logs and access logs are crucial for meeting regulatory requirements and ensuring security by tracking unauthorized access or changes.

- **User Trust and Transparency**: Status pages help maintain user trust by providing transparent communication about service status and incidents.

- **Performance Optimization**: Monitoring metrics like CPU, memory usage, and application performance allows organizations to optimize resource usage and improve system performance.

By integrating these practices into IT operations, organizations can ensure better system reliability, performance, and user satisfaction.

### Metrics on Logs

Metrics on logs are a powerful way to extract meaningful insights from log data, particularly in identifying trends, spikes, or anomalies in system behavior. By aggregating and analyzing log data, you can derive metrics that provide a quantitative view of system performance and health. This approach is especially useful for monitoring errors and understanding the impact of specific issues over time.

#### Key Metrics on Logs

1. **Error Counts**: The simplest metric, counting the number of error messages over a specific period, can help detect when errors are occurring more frequently than expected. This metric can be used to trigger alerts if a threshold is exceeded.

2. **Error Rate**: Error rate metrics provide insight into the frequency of errors relative to the total number of log messages or transactions. This can be helpful in understanding the proportion of errors compared to normal operations.

3. **Spike Detection**: Identifying sudden spikes in error counts can indicate a new issue or a change in system behavior that requires immediate attention. Spike detection metrics are crucial for identifying issues that might not be caught by static thresholds.

4. **Error Distribution**: Analyzing the distribution of different error types can provide insights into the most common issues affecting a system. This helps prioritize debugging efforts based on the frequency and severity of different errors.

5. **Time Series Analysis**: By examining error metrics over time, you can identify patterns or trends, such as whether errors increase during specific periods or after certain deployments. This can help with root cause analysis and preventative maintenance.

### Overview of Monitoring and Observability Services

Monitoring systems are crucial for maintaining the health and performance of applications and infrastructure. They provide insights into various metrics and help detect anomalies, errors, and performance issues. Here’s a detailed look at two popular monitoring setups: **New Relic** and **Prometheus + Grafana + Loki**.

---

### 1. **New Relic**

**New Relic** is a cloud-based observability platform that provides a comprehensive suite of tools for monitoring, troubleshooting, and optimizing the performance of your applications and infrastructure.

#### Key Features:

- **APM (Application Performance Monitoring)**: Monitors application performance and transaction-level details. It helps in understanding the performance of application code and detecting bottlenecks.
- **Infrastructure Monitoring**: Provides visibility into servers, containers, and cloud resources. It includes metrics for CPU, memory, disk usage, and network traffic.
- **Logs**: Centralizes log data, making it easier to correlate logs with metrics and traces for troubleshooting.
- **Alerts and AI-Powered Anomalies**: Set up alerts based on specific thresholds or anomalies detected by New Relic’s AI engine.
- **Dashboards**: Customizable dashboards to visualize data in real-time for quick insights into system health.
- **Synthetic Monitoring**: Simulates user interactions to ensure application availability and performance from different locations.
- **Distributed Tracing**: Provides insights into requests as they flow through distributed systems, helping to identify performance bottlenecks.

#### Use Cases:

- **E-commerce Websites**: Monitoring response times, transaction rates, and error rates to ensure smooth customer experiences.
- **Microservices Architectures**: Tracing and monitoring individual services to optimize communication and performance.
- **DevOps and SRE**: Alerting and anomaly detection to proactively manage incidents and improve Mean Time to Recovery (MTTR).

#### Pricing:

New Relic offers different pricing tiers based on the number of hosts, services monitored, and features used. It is generally considered a premium service and is suitable for organizations that want a comprehensive, all-in-one monitoring solution without managing infrastructure.

---

### 2. **Prometheus + Grafana + Loki**

**Prometheus + Grafana + Loki** is a popular open-source stack for monitoring and observability. It is often used by organizations that prefer self-hosted solutions for greater control over data and customization.

#### Prometheus:

- **Functionality**: Prometheus is an open-source systems monitoring and alerting toolkit. It is designed to record real-time metrics in a time-series database, built using an HTTP pull model, with flexible queries and real-time alerting.
- **Metrics Collection**: Prometheus scrapes metrics from instrumented jobs, either directly or via intermediate push gateways for short-lived jobs.
- **Query Language**: Prometheus has its own query language, PromQL, which allows for powerful data aggregation and analysis.
- **Alerting**: Supports an alerting mechanism that works with the Alertmanager to handle alerts. Alerts can be routed to various destinations such as email, Slack, or custom webhooks.

#### Grafana:

- **Functionality**: Grafana is an open-source analytics and interactive visualization web application. It provides charts, graphs, and alerts for the web when connected to supported data sources.
- **Integration**: Grafana integrates seamlessly with Prometheus and other data sources like Elasticsearch, Graphite, and InfluxDB.
- **Dashboards**: Users can create custom dashboards to visualize metrics in real-time, supporting a variety of visual elements like graphs, heatmaps, and tables.
- **Alerting**: Allows for the creation of alerts based on dashboard metrics, which can be sent to various notification channels.

#### Loki:

- **Functionality**: Loki is a horizontally scalable, highly available, multi-tenant log aggregation system inspired by Prometheus. It is designed to be cost-effective and easy to operate, focusing on indexing only metadata and not full text of the logs.
- **Logs Integration**: Works seamlessly with Grafana for log visualization and analysis, allowing for correlation of logs with metrics.

#### Use Cases:

- **Kubernetes Monitoring**: Prometheus is widely used for monitoring Kubernetes clusters due to its ability to handle highly dynamic environments.
- **Infrastructure Monitoring**: Monitoring server metrics, application performance, and system resources.
- **Cost-Efficiency**: Ideal for organizations looking for a cost-effective, flexible monitoring solution that they can host and customize according to their needs.

#### Benefits:

- **Flexibility**: Highly customizable for specific use cases, allowing users to tailor their monitoring setup to their exact requirements.
- **Community Support**: A large open-source community provides plugins, exporters, and integrations for various platforms and services.
- **Cost-Effective**: No licensing fees as it's open-source, which makes it ideal for organizations with budget constraints.

#### Challenges:

- **Setup and Maintenance**: Requires expertise to set up and maintain. Prometheus, Grafana, and Loki need to be managed and scaled manually.
- **Data Retention**: Prometheus has limited local storage, and users need to manage long-term storage and backups.

---

### Choosing Between New Relic and Prometheus + Grafana + Loki

- **New Relic** is best for organizations that need an all-in-one, managed observability solution with minimal setup. It is suitable for teams that prefer not to handle the infrastructure and want comprehensive support.

- **Prometheus + Grafana + Loki** is ideal for teams that require a highly customizable, self-hosted solution and are comfortable managing their monitoring stack. It offers more flexibility and is cost-effective but requires more expertise and maintenance.

By understanding the strengths and use cases of each, you can choose the monitoring solution that best fits your organization's needs and resources.### Overview of Monitoring and Observability Services

Monitoring systems are crucial for maintaining the health and performance of applications and infrastructure. They provide insights into various metrics and help detect anomalies, errors, and performance issues. Here’s a detailed look at two popular monitoring setups: **New Relic** and **Prometheus + Grafana + Loki**.

---

### 1. **New Relic**

**New Relic** is a cloud-based observability platform that provides a comprehensive suite of tools for monitoring, troubleshooting, and optimizing the performance of your applications and infrastructure.

#### Key Features:

- **APM (Application Performance Monitoring)**: Monitors application performance and transaction-level details. It helps in understanding the performance of application code and detecting bottlenecks.
- **Infrastructure Monitoring**: Provides visibility into servers, containers, and cloud resources. It includes metrics for CPU, memory, disk usage, and network traffic.
- **Logs**: Centralizes log data, making it easier to correlate logs with metrics and traces for troubleshooting.
- **Alerts and AI-Powered Anomalies**: Set up alerts based on specific thresholds or anomalies detected by New Relic’s AI engine.
- **Dashboards**: Customizable dashboards to visualize data in real-time for quick insights into system health.
- **Synthetic Monitoring**: Simulates user interactions to ensure application availability and performance from different locations.
- **Distributed Tracing**: Provides insights into requests as they flow through distributed systems, helping to identify performance bottlenecks.

#### Use Cases:

- **E-commerce Websites**: Monitoring response times, transaction rates, and error rates to ensure smooth customer experiences.
- **Microservices Architectures**: Tracing and monitoring individual services to optimize communication and performance.
- **DevOps and SRE**: Alerting and anomaly detection to proactively manage incidents and improve Mean Time to Recovery (MTTR).

#### Pricing:

New Relic offers different pricing tiers based on the number of hosts, services monitored, and features used. It is generally considered a premium service and is suitable for organizations that want a comprehensive, all-in-one monitoring solution without managing infrastructure.

---

### 2. **Prometheus + Grafana + Loki**

**Prometheus + Grafana + Loki** is a popular open-source stack for monitoring and observability. It is often used by organizations that prefer self-hosted solutions for greater control over data and customization.

#### Prometheus:

- **Functionality**: Prometheus is an open-source systems monitoring and alerting toolkit. It is designed to record real-time metrics in a time-series database, built using an HTTP pull model, with flexible queries and real-time alerting.
- **Metrics Collection**: Prometheus scrapes metrics from instrumented jobs, either directly or via intermediate push gateways for short-lived jobs.
- **Query Language**: Prometheus has its own query language, PromQL, which allows for powerful data aggregation and analysis.
- **Alerting**: Supports an alerting mechanism that works with the Alertmanager to handle alerts. Alerts can be routed to various destinations such as email, Slack, or custom webhooks.

#### Grafana:

- **Functionality**: Grafana is an open-source analytics and interactive visualization web application. It provides charts, graphs, and alerts for the web when connected to supported data sources.
- **Integration**: Grafana integrates seamlessly with Prometheus and other data sources like Elasticsearch, Graphite, and InfluxDB.
- **Dashboards**: Users can create custom dashboards to visualize metrics in real-time, supporting a variety of visual elements like graphs, heatmaps, and tables.
- **Alerting**: Allows for the creation of alerts based on dashboard metrics, which can be sent to various notification channels.

#### Loki:

- **Functionality**: Loki is a horizontally scalable, highly available, multi-tenant log aggregation system inspired by Prometheus. It is designed to be cost-effective and easy to operate, focusing on indexing only metadata and not full text of the logs.
- **Logs Integration**: Works seamlessly with Grafana for log visualization and analysis, allowing for correlation of logs with metrics.

#### Use Cases:

- **Kubernetes Monitoring**: Prometheus is widely used for monitoring Kubernetes clusters due to its ability to handle highly dynamic environments.
- **Infrastructure Monitoring**: Monitoring server metrics, application performance, and system resources.
- **Cost-Efficiency**: Ideal for organizations looking for a cost-effective, flexible monitoring solution that they can host and customize according to their needs.

#### Benefits:

- **Flexibility**: Highly customizable for specific use cases, allowing users to tailor their monitoring setup to their exact requirements.
- **Community Support**: A large open-source community provides plugins, exporters, and integrations for various platforms and services.
- **Cost-Effective**: No licensing fees as it's open-source, which makes it ideal for organizations with budget constraints.

#### Challenges:

- **Setup and Maintenance**: Requires expertise to set up and maintain. Prometheus, Grafana, and Loki need to be managed and scaled manually.
- **Data Retention**: Prometheus has limited local storage, and users need to manage long-term storage and backups.

---

### Choosing Between New Relic and Prometheus + Grafana + Loki

- **New Relic** is best for organizations that need an all-in-one, managed observability solution with minimal setup. It is suitable for teams that prefer not to handle the infrastructure and want comprehensive support.

- **Prometheus + Grafana + Loki** is ideal for teams that require a highly customizable, self-hosted solution and are comfortable managing their monitoring stack. It offers more flexibility and is cost-effective but requires more expertise and maintenance.

By understanding the strengths and use cases of each, you can choose the monitoring solution that best fits your organization's needs and resources.



#### Example SQL Queries for Metrics on Logs

Let's explore some SQL queries to illustrate how to implement metrics on logs, specifically focusing on error detection.

1. **Counting Error Messages**:
   This query counts the number of error messages generated by specific entities or services, helping to identify if there is a high volume of errors that might need attention.

   ```sql
   SELECT count(`message`) 
   FROM Log 
   WHERE 
       (`entity.guid` = 'NDQ2MDY2NXxBUE18QVBQTElDQVRJT058NTY0ODg2NzU5' 
        OR `entity.guids` LIKE '%NDQ2MDY2NXxBUE18QVBQTElDQVRJT058NTY0ODg2NzU5%' 
        OR `service_name` = 'test' 
        OR `serviceName` = 'test' 
        OR `service.name` = 'test' 
        OR `entity.name` = 'test') 
       AND (level = 'error')
   TIMESERIES AUTO
   ```

   - **Explanation**: This query filters logs based on specific entity GUIDs or service names and looks for logs marked as `error`. The `TIMESERIES AUTO` clause helps to visualize the error count over time, allowing for detection of trends or spikes.

2. **Detecting Specific Error Patterns**:
   This query adds another layer of filtering by searching for specific error messages. This is useful when you want to monitor a known error condition or investigate the frequency of a particular issue.

   ```sql
   SELECT count(`message`) 
   FROM Log 
   WHERE 
       (`entity.guid` = 'NDQ2MDY2NXxBUE18QVBQTElDQVRJT058NTY0ODg2NzU5' 
        OR `entity.guids` LIKE '%NDQ2MDY2NXxBUE18QVBQTElDQVRJT058NTY0ODg2NzU5%' 
        OR `service_name` = 'test' 
        OR `serviceName` = 'test' 
        OR `service.name` = 'test' 
        OR `entity.name` = 'test') 
       AND (level = 'error') 
       AND message LIKE '%there was an error%'
   TIMESERIES AUTO
   ```

   - **Explanation**: This query is similar to the previous one but adds a condition to only count error messages that contain the phrase "there was an error". This is useful for tracking specific known errors and understanding their frequency and impact.

3. **Metrics on Individual Errors**:
   You can create metrics for individual error types by specifying different error messages or conditions. For example:

   ```sql
   SELECT count(`message`) AS error_count, `message`
   FROM Log 
   WHERE 
       (`entity.guid` = 'NDQ2MDY2NXxBUE18QVBQTElDQVRJT058NTY0ODg2NzU5' 
        OR `entity.guids` LIKE '%NDQ2MDY2NXxBUE18QVBQTElDQVRJT058NTY0ODg2NzU5%' 
        OR `service_name` = 'test' 
        OR `serviceName` = 'test' 
        OR `service.name` = 'test' 
        OR `entity.name` = 'test') 
       AND (level = 'error') 
       AND (
           message LIKE '%database connection failed%' 
           OR message LIKE '%timeout occurred%' 
           OR message LIKE '%invalid input%'
       )
   GROUP BY `message`
   ORDER BY error_count DESC
   ```

   - **Explanation**: This query groups errors by their message content, providing a count for each unique error type. This allows you to identify the most common errors and prioritize them based on frequency.

### Benefits of Adding Metrics to Logs

- **Improved Visibility**: Metrics provide a high-level overview of system health, allowing teams to quickly identify abnormal behavior without sifting through raw log data.

- **Proactive Issue Detection**: By monitoring metrics, you can detect potential issues before they become critical, enabling proactive maintenance and reducing downtime.

- **Efficient Debugging**: Knowing the frequency and context of errors helps narrow down potential causes, making debugging more efficient and focused.

- **Resource Optimization**: Understanding trends in error rates and system performance helps in optimizing resources and planning for scaling.

- **Compliance and Reporting**: Metrics help in generating reports for compliance purposes, showing historical data of system performance and incidents.

By integrating metrics with logging data, organizations can enhance their monitoring capabilities, enabling more proactive and informed decision-making to ensure system stability and performance.

### Understanding Metrics for Performance Measurement: Mean, Median, and Percentiles

When measuring metrics like response times, CPU usage, or other performance indicators, choosing the right statistical metric is crucial to accurately understanding and interpreting system behavior. The decision between using the mean (average), median, or percentiles depends on the distribution of the data and the presence of outliers.

#### Mean (Average)

- **Definition**: The mean is the sum of all values divided by the number of values. 
- **Use Case**: The mean is useful for datasets that are normally distributed, where values are evenly spread around the central value without significant outliers.
- **Limitations**: The mean is sensitive to outliers. In a dataset with extreme values, the mean can be skewed, leading to a misleading representation of the typical values.

#### Median

- **Definition**: The median is the middle value of a dataset when it is ordered from least to greatest. If there is an even number of values, the median is the average of the two middle values.
- **Use Case**: The median is useful when the dataset has outliers or is not symmetrically distributed. It represents the middle point, meaning half the data points are above and half are below this value.
- **Advantages**: The median is not affected by outliers, making it a more reliable measure of central tendency for skewed distributions.

#### Percentiles (P95, P99, etc.)

- **Definition**: Percentiles are measures that indicate the value below which a given percentage of observations fall. For example, the 95th percentile (P95) is the value below which 95% of the data points are found.
- **Use Case**: Percentiles are especially useful for understanding the distribution of response times, CPU usage, or other performance metrics where outliers are significant. They provide a more granular view of performance, showing how extreme cases affect the overall experience.
- **Advantages**: Percentiles like P95 or P99 help identify performance for the majority of users and pinpoint outliers. This is crucial in systems where even a small percentage of slow responses or high CPU usage can indicate underlying issues that need addressing.

### Example: Analyzing Response Times

Given a set of response times:
`[1, 2, 3, 4, 4, 4, 5, 6, 7, 9, 10, 11, 11, 11, 12, 13, 13, 13, 50, 100]`

- **Average (Mean)**: 14.45  
  The mean is heavily influenced by the outliers (50 and 100), which skews the average higher than most individual response times.
  
- **Median**: 9.5  
  The median provides a better central value, unaffected by the outliers, representing the 50th percentile.

- **Percentiles**:  
  - **P90**: 13 (90% of requests have a response time of 13ms or lower)
  - **P95**: 50 (95% of requests have a response time of 50ms or lower)

These percentiles indicate that while most users experience good response times, a small percentage (5%) experience much higher response times due to outliers.

### Guessing P95 and P99 for the Example Data

Based on the data provided:

- **P95**: To calculate the 95th percentile, sort the list and find the value below which 95% of the data falls.  
  With 20 values, the position is `20 * 0.95 = 19`. The 19th value is 50, so **P95 = 50**.

- **P99**: For the 99th percentile, in a list of 20 values, position would be `20 * 0.99 = 19.8`. Since there isn't a 20th value to average, we approximate with the highest value in the data. **P99 = 100**.

### Percentile-Based Metrics in New Relic

In New Relic's APM dashboard, percentile metrics help monitor application performance:

- **P95 (95th Percentile)**: Shows response time for 95% of transactions. A higher P95 can indicate that a significant minority of requests are much slower than the rest, often pointing to performance bottlenecks or resource issues.
- **P99 (99th Percentile)**: Similar to P95 but for 99% of transactions. Useful for identifying extreme outliers that can significantly affect the user experience for a few users.
- **Median**: Offers insight into the typical user experience, unaffected by extreme outliers.
- **Average**: Useful for a general sense of overall performance but less reliable for skewed data.

### Example Query in New Relic

The provided SQL-like query in New Relic retrieves various metrics:

```sql
SELECT 
    percentile(duration, 95) * 1000 AS P95,
    percentile(duration, 99) * 1000 AS P99,
    median(duration * 1000) AS Median,
    average(duration * 1000) AS Average
FROM 
    Transaction
WHERE 
    (entityGuid = 'NDQ2MDY2NXxBUE18QVBQTElDQVRJT058NTY0ODg2NzU5')
    AND (transactionType = 'Web')
LIMIT MAX 
SINCE 1800 seconds AGO 
EXTRAPOLATE 
TIMESERIES
```

- **`percentile(duration, 95) * 1000`**: Calculates the 95th percentile of the response time (`duration`) in milliseconds.
- **`percentile(duration, 99) * 1000`**: Calculates the 99th percentile of the response time in milliseconds.
- **`median(duration * 1000)`**: Calculates the median response time in milliseconds.
- **`average(duration * 1000)`**: Calculates the average response time in milliseconds.
- **`WHERE (entityGuid = 'NDQ2MDY2NXxBUE18QVBQTElDQVRJT058NTY0ODg2NzU5')`**: Filters the transactions to a specific entity.
- **`AND (transactionType = 'Web')`**: Further filters transactions to web requests.
- **`LIMIT MAX`**: Ensures all relevant data points are included in the results.
- **`SINCE 1800 seconds AGO`**: Limits the query to data from the last 30 minutes (1800 seconds).
- **`EXTRAPOLATE TIMESERIES`**: Helps create a time series data visualization.

By leveraging these percentile metrics, New Relic provides a detailed view of system performance, enabling teams to proactively address performance issues and improve user experience.

### Setting Up Logging, Monitoring, and Alerting in New Relic

New Relic offers a comprehensive observability platform that allows you to monitor your applications and infrastructure in real-time, collect and analyze logs, and set up alerts to proactively manage your systems. Here's a detailed guide on setting up **Logging**, **Monitoring**, and **Alerting** in New Relic.

#### 1. **Logging in New Relic**

Logging in New Relic involves setting up log ingestion so that you can collect, visualize, and analyze log data from your systems. Follow these steps to set up logging:

1. **Log in to New Relic**:
   - Navigate to the New Relic login page and enter your credentials to access the platform.

2. **Select Your Stack**:
   - From the main dashboard, choose the technology stack you want to monitor. This could be based on the type of application you are running (e.g., Node.js, Python, Java, etc.).

3. **Select Machine (OS) to Install It On**:
   - Depending on your server's operating system (e.g., Linux, Windows), select the appropriate option for installing the New Relic agent. This agent will be responsible for sending log data to New Relic.

4. **Create a New API Key**:
   - You need to create a new API key that will be used by the agent to authenticate and send data to New Relic. This can typically be done in the "Account Settings" or "API Keys" section of the platform.

5. **Run the Command on Your Machine**:
   - New Relic will provide a command to install the agent on your server. This command usually includes the API key and specific installation parameters. Run this command on your server to start sending logs to New Relic.

6. **Verify Log Ingestion**:
   - After installation, go to the Logs section in New Relic and check if your logs are being ingested correctly. You can filter, search, and analyze logs from this interface.

#### 2. **Monitoring in New Relic**

Monitoring in New Relic involves observing the health and performance of your hosts, applications, and infrastructure. Here’s how you can set it up:

1. **Navigate to Hosts**:
   - Go to the "Entities" section and select "Hosts" to see an overview of all the hosts you are monitoring.

2. **Install the Infrastructure Agent**:
   - If not already installed, set up the New Relic Infrastructure agent on your servers. This agent collects detailed metrics about your servers, including CPU usage, memory usage, disk I/O, and more.

3. **Explore Host Metrics**:
   - Once the agent is installed, you can view various metrics for each host. This includes system-level metrics and application performance data, giving you complete observability of your environment.

4. **Experiment with Metrics**:
   - To see how metrics appear in New Relic, you can simulate load or errors. For example, create an `index.js` file with an infinite loop to simulate CPU load:
   ```javascript
   let c = 1;
   while (1) {
       if (Math.random() < 0.1)
           c = c + 1;
   }
   ```
   - Run this script on your server and observe how New Relic tracks the CPU usage spike.

5. **Create Dashboards**:
   - Go to the **Dashboards** section in New Relic to create custom dashboards. You can add widgets to visualize different metrics, such as CPU usage, memory consumption, network traffic, and more.

6. **Adding Widgets to Dashboards**:
   - Add widgets like line charts, bar graphs, and heat maps to display the metrics you are monitoring. Customize these widgets to suit your needs and give you at-a-glance insights into your system’s performance.

#### 3. **Alerting in New Relic**

Alerting is crucial for proactive system management. It helps you detect and respond to issues before they impact your users. Here’s how to set up alerts:

1. **Create an Alert Condition**:
   - Go to the "Alerts & AI" section and click on "Create a condition." Here, you can define conditions based on metrics or log data. For example, you can set an alert for CPU usage exceeding a certain threshold or for a specific error appearing frequently in logs.

2. **Attach the Alert to a Policy**:
   - Alerts need to be attached to a policy. A policy is a collection of alert conditions and the associated notification channels. Create a new policy or attach your alert condition to an existing policy.

3. **Add Notification Channels**:
   - To ensure the right people are informed when an alert is triggered, add notification channels to your policy. New Relic supports various notification methods, including email, Slack, PagerDuty, and webhooks.
   
   - To add a notification channel, go to the "Notification Channels" section and add your preferred method. Then, link this channel to your alert policy.

4. **Test Your Alerts**:
   - It's a good practice to test your alerts to make sure they are working as expected. Trigger a known alert condition, like increasing CPU usage, and verify that the alert notification is sent to your configured channels.

#### **Assignment**

To get hands-on experience, try creating a New Relic dashboard with the following widgets:

- **CPU Usage**: Add a widget to monitor CPU usage across all your hosts.
- **Memory Usage**: Add a widget to display memory consumption.
- **Error Rates**: Create a graph showing error rates over time.
- **Custom Script Monitoring**: Monitor the performance impact of the `index.js` infinite loop script.

Additionally, set up an alert for high CPU usage, attach it to a policy, and configure a notification channel (like email or Slack) to receive alerts when the condition is met.

By following these steps, you'll be able to effectively monitor, log, and alert on your systems using New Relic, ensuring you have full visibility and control over your environment.

### NRQL (New Relic Query Language) Basics

NRQL (New Relic Query Language) is a powerful SQL-like query language designed to query and analyze data stored within New Relic. It allows you to create custom queries for generating insights, visualizations, and alerts based on your application and infrastructure performance data. Here’s an overview of some common NRQL queries, focusing on CPU usage and network performance.

#### 1. **Fetching All CPU Usage**

To monitor the CPU usage of a specific entity, you can use the `SystemSample` event type in New Relic. The following query fetches the average CPU usage percentage for a given entity over time.

**NRQL Query:**
```sql
SELECT average(cpuPercent) AS `CPU used %` 
FROM SystemSample 
WHERE (entityGuid = 'xyz') 
TIMESERIES AUTO
```

- **`average(cpuPercent)`:** This calculates the average CPU percentage used.
- **`entityGuid`:** A unique identifier for the entity (server or host) whose CPU usage you want to monitor.
- **`TIMESERIES AUTO`:** This clause automatically selects the best time interval for the data points.

#### 2. **Filtering by High CPU Usage**

To filter and display only the data points where CPU usage is above a certain threshold (e.g., 2%), you can modify the query to include a `WHERE` clause that filters based on `cpuPercent`.

**NRQL Query:**
```sql
SELECT average(cpuPercent) AS `CPU used %`
FROM SystemSample
WHERE (entityGuid = 'NDQ2MDY2NXxJTkZSQXxOQXwzOTczMjM4ODc0NzI4NDk0NzYz') 
AND cpuPercent > 2
TIMESERIES AUTO
```

- **`cpuPercent > 2`:** This condition filters out any data points where CPU usage is 2% or lower.

#### 3. **Multiple Graphs in the Same Timeline**

To visualize multiple metrics (like network transmit and receive bytes per second) on the same timeline, you can include multiple calculations in a single NRQL query.

**NRQL Query:**
```sql
SELECT average(transmitBytesPerSecond) AS `Transmit bytes per second`, 
       average(receiveBytesPerSecond) AS `Receive bytes per second` 
FROM NetworkSample 
WHERE (entityGuid = 'NDQ2MDY2NXxJTkZSQXxOQXwtMzE2MTI3OTkyNzM3NDEzMTE1Mw') 
TIMESERIES AUTO
```

- **`average(transmitBytesPerSecond)`** and **`average(receiveBytesPerSecond)`**: These calculate the average transmit and receive bytes per second.
- This query provides two separate graphs on the same timeline, showing how network performance metrics vary over time.

#### 4. **Using Facets to Compare Data**

Facets in NRQL allow you to break down and compare data by different dimensions. For example, you can compare CPU usage across multiple entities (hosts) by using the `FACET` clause.

**NRQL Query:**
```sql
SELECT average(cpuPercent) AS `CPU used %`
FROM SystemSample
WHERE entityGuid IN ('xyz', 'abc')
FACET entityGuid
TIMESERIES AUTO
```

- **`FACET entityGuid`:** This clause groups the results by `entityGuid`, allowing you to compare the average CPU usage across multiple entities (in this case, `xyz` and `abc`).

### Tips for Using NRQL

- **Leverage Timeseries:** Use `TIMESERIES` to break down data over time, which is useful for identifying trends and spikes.
- **Faceting:** Use `FACET` to compare data across different entities or attributes.
- **Filters:** Apply filters (`WHERE` clause) to focus on specific data points or conditions.
- **Alerts:** Combine NRQL queries with New Relic alerts to create proactive notifications based on your monitoring data.

By mastering these NRQL basics, you can effectively monitor your infrastructure and application performance, create detailed dashboards, and set up meaningful alerts in New Relic.
